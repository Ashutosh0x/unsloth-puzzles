{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Unsloth Puzzles: All Challenges\n",
                "\n",
                "This notebook contains implementations and benchmarks for Challenges A, B, and C."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch>=2.4 transformers peft trl accelerate bitsandbytes unsloth -U --quiet"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Challenge A: NF4 Triton Kernel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import triton\n",
                "import triton.language as tl\n",
                "import time\n",
                "from unsloth.kernels import fast_dequantize\n",
                "from bitsandbytes.nn import LinearNF4\n",
                "\n",
                "@triton.jit\n",
                "def _your_dequantize_nf4_kernel(\n",
                "    weight_ptr,\n",
                "    absmax_ptr,\n",
                "    code_ptr,\n",
                "    out_ptr,\n",
                "    n_elements,\n",
                "    BLOCK_SIZE: tl.constexpr,\n",
                "):\n",
                "    pid = tl.program_id(0)\n",
                "    block_start = pid * BLOCK_SIZE\n",
                "    \n",
                "    byte_offsets = (block_start // 2) + tl.arange(0, BLOCK_SIZE // 2)\n",
                "    mask = byte_offsets < (n_elements // 2)\n",
                "    \n",
                "    packed_weights = tl.load(weight_ptr + byte_offsets, mask=mask)\n",
                "    \n",
                "    low_nibble = (packed_weights & 0xF).to(tl.int32)\n",
                "    high_nibble = (packed_weights >> 4).to(tl.int32)\n",
                "    \n",
                "    # Note: Use tl.load from ptr for indexing\n",
                "    val_low = tl.load(code_ptr + low_nibble)\n",
                "    val_high = tl.load(code_ptr + high_nibble)\n",
                "    \n",
                "    abs_low = tl.load(absmax_ptr + (block_start + tl.arange(0, BLOCK_SIZE // 2) * 2) // 64, mask=mask)\n",
                "    abs_high = tl.load(absmax_ptr + (block_start + tl.arange(0, BLOCK_SIZE // 2) * 2 + 1) // 64, mask=mask)\n",
                "    \n",
                "    val_low = val_low * abs_low\n",
                "    val_high = val_high * abs_high\n",
                "    \n",
                "    out_offsets_low = block_start + tl.arange(0, BLOCK_SIZE // 2) * 2\n",
                "    out_offsets_high = out_offsets_low + 1\n",
                "    \n",
                "    tl.store(out_ptr + out_offsets_low, val_low, mask=mask)\n",
                "    tl.store(out_ptr + out_offsets_high, val_high, mask=mask)\n",
                "\n",
                "def your_dequantize_nf4(linear):\n",
                "    weight = linear.weight.data\n",
                "    quant_state = linear.weight.quant_state\n",
                "    n_elements = weight.numel() * 2\n",
                "    out = torch.empty(quant_state.shape, dtype=quant_state.dtype, device=weight.device)\n",
                "    BLOCK_SIZE = 1024\n",
                "    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)\n",
                "    _your_dequantize_nf4_kernel[grid](\n",
                "        weight, quant_state.absmax, quant_state.code, out, n_elements, BLOCK_SIZE=BLOCK_SIZE,\n",
                "    )\n",
                "    return out\n",
                "\n",
                "# Correctness & Speed Check\n",
                "device = \"cuda\"\n",
                "shape = (4096, 4096)\n",
                "linear = LinearNF4(shape[1], shape[0], bias=False).to(device)\n",
                "out_ref = fast_dequantize(linear.weight.data, linear.weight.quant_state)\n",
                "out_custom = your_dequantize_nf4(linear)\n",
                "print(f\"A Correctness: {torch.allclose(out_ref, out_custom, atol=1e-5)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Challenge B: FSDP2 + QLoRA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from datasets import load_dataset\n",
                "\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
                "model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
                "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map={ \"\": 0 }, torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\")\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "lora_config = LoraConfig(r=64, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], task_type=\"CAUSAL_LM\")\n",
                "model = get_peft_model(model, lora_config)\n",
                "\n",
                "dataset = load_dataset(\"philschmid/dolly-15k-llama-3-format\", split=\"train[:100]\")\n",
                "training_args = SFTConfig(\n",
                "    output_dir=\"./outputs\", per_device_train_batch_size=1, gradient_accumulation_steps=1,\n",
                "    max_steps=5, bf16=True, dataset_text_field=\"text\", max_seq_length=128,\n",
                "    fsdp=\"full_shard auto_wrap\",\n",
                "    fsdp_config={\n",
                "        \"fsdp_transformer_layer_cls_to_wrap\": \"LlamaDecoderLayer\",\n",
                "        \"activation_checkpointing\": True, \"offload_params\": True,\n",
                "    },\n",
                ")\n",
                "trainer = SFTTrainer(model=model, train_dataset=dataset, args=training_args)\n",
                "trainer.train()\n",
                "print(\"B Completed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Challenge C: torch.compile"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch._dynamo\n",
                "@torch.compile(fullgraph=False, dynamic=True)\n",
                "def compiled_forward(model, input_ids, labels):\n",
                "    return model(input_ids=input_ids, labels=labels).loss\n",
                "\n",
                "input_ids = torch.randint(0, 32000, (1, 128)).cuda()\n",
                "print(\"Checking C graph breaks...\")\n",
                "explanation = torch._dynamo.explain(compiled_forward, model, input_ids, input_ids)\n",
                "print(f\"Graph breaks: {explanation.graph_break_count}\")\n",
                "print(\"C Completed!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}