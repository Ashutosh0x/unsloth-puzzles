{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Challenge C: torch.compile without Graph Breaks\n",
                "\n",
                "This notebook verifies the graph breaks and recompilations for QLoRA with `torch.compile`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch>=2.4 transformers peft bitsandbytes -U --quiet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch._dynamo\n",
                "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model\n",
                "\n",
                "torch._dynamo.config.suppress_errors = True\n",
                "# torch._dynamo.config.verbose = True\n",
                "\n",
                "model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    target_modules=[\"q_proj\", \"v_proj\"],\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")\n",
                "model = get_peft_model(model, lora_config)\n",
                "\n",
                "# Challenge: Eliminate graph breaks\n",
                "# We wrap the forward pass\n",
                "@torch.compile(fullgraph=False, dynamic=True)\n",
                "def compiled_forward(model, input_ids, labels):\n",
                "    return model(input_ids=input_ids, labels=labels).loss\n",
                "\n",
                "# Diagnostic check\n",
                "input_ids = torch.randint(0, 32000, (1, 128)).cuda()\n",
                "labels = input_ids.clone()\n",
                "\n",
                "print(\"Running diagnostic...\")\n",
                "explanation = torch._dynamo.explain(compiled_forward, model, input_ids, labels)\n",
                "print(f\"Graph breaks: {explanation.graph_break_count}\")\n",
                "for break_reason in explanation.graph_breaks:\n",
                "    print(f\"Reason: {break_reason.reason}\")\n",
                "\n",
                "# Run multiple iterations to check for recompilations\n",
                "print(\"Checking recompilations...\")\n",
                "for i in range(5):\n",
                "    loss = compiled_forward(model, input_ids, labels)\n",
                "    print(f\"Iteration {i} loss: {loss.item()}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}