{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Challenge B: FSDP2 + QLoRA on 2xT4 GPUs\n",
                "\n",
                "This notebook demonstrates finetuning a model using QLoRA with FSDP2 on Kaggle's 2x Tesla T4 GPUs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch>=2.4 transformers peft trl accelerate bitsandbytes -U --quiet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Must be 2xT4\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
                "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
                "\n",
                "model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map={ \"\": 0 }, \n",
                "    torch_dtype=torch.bfloat16,\n",
                "    attn_implementation=\"sdpa\",\n",
                ")\n",
                "\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "lora_config = LoraConfig(\n",
                "    r=64,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")\n",
                "model = get_peft_model(model, lora_config)\n",
                "\n",
                "# Dataset\n",
                "dataset = load_dataset(\"philschmid/dolly-15k-llama-3-format\", split=\"train\")\n",
                "\n",
                "training_args = SFTConfig(\n",
                "    output_dir=\"./outputs\",\n",
                "    per_device_train_batch_size=2,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    bf16=True, \n",
                "    logging_steps=1,\n",
                "    max_steps=10,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=512,\n",
                "    # FSDP2 config\n",
                "    fsdp=\"full_shard auto_wrap\",\n",
                "    fsdp_config={\n",
                "        \"fsdp_transformer_layer_cls_to_wrap\": \"LlamaDecoderLayer\",\n",
                "        \"activation_checkpointing\": True,\n",
                "        \"forward_prefetch\": True,\n",
                "        \"offload_params\": True,\n",
                "    },\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    args=training_args,\n",
                ")\n",
                "\n",
                "trainer.train()\n",
                "print(\"FSDP2 + QLoRA Training completed!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}